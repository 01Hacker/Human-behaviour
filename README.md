# Human-behaviour
Paper notes  
**How behavioural sciences can promote truth, autonomy and democratic discourse online**
**abstract:**  
<p align="justify">To the extent that a “wealth of information creates a poverty of attention” (p. 41)[1], people have never been as cognitively impoverished as they are today. Major web platforms such as Google and Facebook serve as hubs, distributors and curators[2]; their algorithms are indispensable for navigating the vast digital landscape and for enabling bottom-up participation in the production and distribution of information. Technology companies exploit this all-important role in pursuit of the most precious resource in the online marketplace: human attention. Employing algorithms that learn people’s behavioural patterns[3–6], such companies target their users with advertisements and design users’ information and choice environments[7]. The relationship between platforms and people is profoundly asymmetric: platforms have deep knowledge of users’ behaviour, whereas users know little about how their data is collected, how it is exploited for commercial or political purposes, or how it and the data of others are used to shape their online experience.</p>   
<p align="justify">These asymmetries in Big Tech’s business model have created an opaque information ecology that undermines not only user autonomy but also the transparent exchange on which democratic societies are built[8,9].</p>  
<p align="justify">Several problematic social phenomena pervade the internet, such as the spread of false information[10–14]—which includes disinformation (intentionally fabricated falsehoods) and misinformation (falsehoods created without intent, for example, poorly researched content or biased reporting)—or attitudinal and emotional polarization[15,16] (for example, polarization of elites[17], partisan sorting[18] and polarization with respect to controversial topics[19,20]).</p>   
<p align="justify">The role of behavioural science is not only to advance active scientific debates on the causes and reach of false information[21–25] or on whether mass polarization is increasing[26–28]; it is also to find new ways to promote the Internet’s potential to bolster rather than undermine democratic societies[29]. Solutions to many global problems—from climate change to the coronavirus pandemic—require coordinated collective solutions, making a democratically interconnected world crucial[30].</p>  
<p align="justify">More than any traditional media, online media permit and encourage active behaviours[31] such as information search, interaction and choice. These behaviours are highly contingent on environmental and social structures and cues[32].</p>  
<p align="justify">For instance, curtailing the number of times a message can be forwarded on WhatsApp (thereby slowing large cascades of messages) may have been a successful response to the spread of misinformation in Brazil and India[33].</p>  
<p align="justify">Today, more than half (55%) of global internet users turn to either social media or search engines to access news articles[2].</p>  
<p align="justify">One implication of this seismic shift is that a small number of global corporations and Silicon Valley CEOs have significant responsibility for curating the general population’s information[34] and, by implication, for interpreting discussions of major policy questions and protecting civic freedoms. </p>  
<p align="justify">The current situation, in which political content and news diets are curated by opaque and largely unaccountable third parties, is considered unacceptable by a majority of the public[35,36], who continue to be concerned about their ability to discern online what is true and what is false[2] and who rate accuracy as a very important attribute for social media sharing[37].</p>  
<p align="justify">How can citizens and democratic governments be empowered[38] to create an ecosystem that “values and promotes truth”(p. 1096)[14]? The answers must be informed by independent behavioural research, which can then form the basis both for improved self-regulation by the relevant companies and for government regulation[39,40]. </p>  
<p align="justify">An example comes from most of the current consent forms under the European Union (EU) General Data Protection Regulation: instead of obtaining genuinely informed consent, the dialogue boxes influence people’s decision-making through self-serving forms of choice architecture (for example, consent is assumed from pre-ticked boxes or inactivity)[41,42].</p>  
**Regulators in particular face three serious problems in the online domain that underscore the importance of enlisting the behavioural sciences.**  
- The first problem is that online platforms can leverage their proprietary knowledge of user behaviour to defang regulations.   
- The second problem is that the speed and adaptability of technology and its users exceed that of regulation directly targeting online content.   
- The third problem is the risk of censorship inherent in regulations that target content; behavioural sciences can reduce that risk as well.   
<p align="justify">Rather than deleting or flagging posts based on judgements about their content, we focus here on how to redesign digital environments so as to provide a better sense of context and to encourage and empower people to make critical decisions for themselves[43–45].</p>  
Our aim is to enlist **two streams of research** that illustrate the promise of behavioural sciences.  
<p align="justify"> - The first examines the informational cues that are available online31 and asks which can help users gauge the epistemic quality of content or the trustworthiness of the social context from which it originated.</p>  
<p align="justify"> - The second stream concerns the use of meaningful and predictive cues in behavioural interventions. Interventions can take the form of nudging[46], which alters the environment or choice architecture so as to draw users’ attention to these cues, or boosting[47], which teaches users to search for them on their own, thereby helping them become more resistant to false information and manipulation, especially but not only in the long run.</p>  
<p align="justify">The online world has the potential to provide digital cues that can help people assess the epistemic quality of content[48–50]—the potential of self-contained units of information (here we focus on online articles and social media posts) to contribute to true beliefs, knowledge and understanding—and the public’s attitudes to societal issues[51,52]. We classify those cues as **endogenous or exogenous**[53].</p>  
<p align="justify">Modern search engines use natural language-processing tools that analyse content[54]. Such tools have considerable virtues and promise, but current results rarely afford nuanced interpretations[55]. For example, these methods cannot reliably distinguish between facts and opinions, nor can they detect irony, humour or sarcasm[56]. They also have difficulty differentiating between extremist content and counter-extremist messages[57], because both types of messages tend to be tagged with similar keywords.</p>  
<p align="justify">Both nudging and boosting have been shown to be effective in various domains, including health58,59 and finances[60]. Recent empirical results from research on people’s ability to detect false news indicate that informational literacy can also be boosted[61]. Initial results on the effectiveness of simple nudging interventions that remind people to think about accuracy before sharing content[37] also suggest that such interventions can be effective in the online domain[62].</p>  
<p align="justify">The capacity to transfer information online continues to increase exponentially (average annual growth rate: 28%)[63]. Content can be distributed more rapidly and reaches an audience faster[64].</p>  
<p align="justify">The same declining half-life has been observed for Google queries and movie ticket sales[65]. This acceleration, arguably driven by the finite limits of attention available for the ever-increasing quantity of topics and content[66] alongside an apparent thirst for novelty, has significant but underappreciated psychological consequences. Information overload makes it harder for people to make good decisions about what to look at, spend time on, believe and share[67,68]. </p>  
<p align="justify">The more sources crowd the market, the less attention can be allocated to each piece of content and the more difficult it becomes to assess the trustworthiness of each—even more so given the demise and erosion of classic indicators of quality[69] (for example, name recognition, reputation, print quality, price). </p>  
<p align="justify">To help users navigate the overabundance of information, search engines automatically order results[70,71], and recommender systems72 guide users to content they are likely to prefer[73]. But this convenience exacts a price. Because user satisfaction is not necessarily in line with the goals of algorithms—to maximize user engagement and screen time[74]—algorithmic curation often deprives users of autonomy. For instance, feedback loops are created that can artificially reinforce preferences[75–78], and recommender systems can eliminate context in order to avoid overburdening users. To stay up-to-date and engaging, algorithms can trade recency for importance[79] and, by optimizing on click rates, trade ‘clickbait’ for quality.</p>  
<p align="justify">Similarly, aggregated previous user selections make targeted commercial nudging—and even manipulation—possible[80,81]. For example, given just 300 Facebook likes from one person, a regression model can better predict that person’s personality traits than friends and family[82].</p>  
<p align="justify">Recent surveys in the USA and Germany found that a majority of respondents consider such data-driven personalization of political content (61%), social media feeds (57%) and news diets (51%) unacceptable, whereas they are much more accepting of it when it pertains to commercial content[35,36].</p>  
<p align="justify">More than two thirds of all internet users (around 3 billion people) actively use social media[83]. These platforms offer information about the behaviour of others (for example, likes and emoticons)[84] and new opportunities for interaction (for example, follower relationships and comment sections). However, these signals and interactions are often one-dimensional, represent only a user’s immediate online neighbourhood and do not distinguish between different types of connections[85]. These limitations can have drastic effects, such as dramatically changing a user’s perception of group sizes[86,87] and giving rise to false-consensus effects (i.e., the majority opinion in an individual’s neighbourhood leads people wrongly to believe it reflects the actual majority opinion; Fig. 1b). When people associate with like-minded others from a globally dispersed online community, their self-selected social surroundings (known as a homophilic social network) and the low visibility of the global state of the network[88,89] can create the illusion of broad support[90] and reinforce opinions or even make them more extreme[91,92]. For instance, even if only a tiny fraction (for example, one in a million) of the more than two billion Facebook users believe that the Earth is flat, they could still form an online community of thousands, thereby creating a shield of like-minded people against corrective efforts[93–96]. </p>  
<p align="justify">Although large social media platforms routinely aggregate information that would foster a realistic assessment of societal attitudes, they currently do not provide a well-calibrated impression of the degree of public consensus[97]. Instead, they show reactions from others as asymmetrically positive—there typically is no ‘dislike’ button—or biased toward narrow groups or highly active users[98] to maximize user engagement. This need not be the case. The interactive nature of social media could be harnessed to promote diverse democratic dialogue and foster collective intelligence. To achieve this goal, social media needs to offer more meaningful, higher-dimensional cues that carry information about the broader state of the network rather than just the user’s direct neighbourhood, which can mitigate biased perceptions caused by the network structure[99]. For instance, social media platforms could provide a transparent crowd-sourced voting system[100] or display informative metrics about the behaviour and reactions of others (for example, including passive behaviour, like the total number of people who scrolled over a post), which might counter false-consensus effects. </p>  
<p align="justify"> For instance, highlighting when content stems from few or anonymous sources (as used by Wikipedia) can remind people to scrutinize content more thoroughly[101,102] and simultaneously create an incentive structure for content producers to meet the required criteria. Such outlets can be made more transparent, for example by disclosing the identity of their confirmed owners. Similarly, pages that are run by state-controlled media might be labelled as such[103]. Going a step further, adding prominent hyperlinks to vetted reference sources for important concepts in a text could encourage a reader to gain context by perusing multiple sources—a strategy used by professional fact checkers[104].</p>  
<p align="justify">Nudges can also communicate additional information about what others are doing, thereby invoking the steering power of descriptive social norms[105]:</p>  
<p align="justify">Transparent numerical formats have already been shown to improve statistical literacy in the medical domain[106].</p>  
<p align="justify">Similarly, users could be discouraged from sharing low-quality information without resorting to censorship by introducing ‘sludge’ or ‘friction’—for instance, by making the act of sharing slightly more effortful[107].</p>  
<p align="justify"> Indeed, news feeds have become one of the most sophisticated algorithmically driven choice architectures of online platforms[7,108]</p>  
<p align="justify">Boosting seeks to empower people in the longer term by helping them build the competences they need to navigate situations autonomously (for a conceptual map of boosting interventions online, see also ref. [109]). </p>  
<p align="justify">The competence of acting as one’s own choice architect, or self-nudging, can be boosted[110]. </p>  
<p align="justify">Another competence that could be boosted to help users deal more expertly with information they encounter online is the ability to make inferences about the reliability of information based on the social context from which it originates[111]. The structure and details of the entire cascade of individuals who have previously shared an article on social media has been shown to serve as proxies for epistemic quality[112]. More specifically, the sharing cascade contains metrics such as the depth and breadth of dissemination by others, with deep and narrow cascades indicating extreme or niche topics and breadth indicating widely discussed issues[113]. </p>  
<p align="justify">Yet another competence required for distinguishing between sources of high and low quality is the ability to read laterally[104]. Lateral reading is a skill developed by professional fact checkers that entails looking for information on sites other than the information source in order to evaluate its credibility (for example, ‘who is behind this website?’ and ‘what is the evidence for its claims?’) rather than evaluating a website’s credibility by using the information provided there. This competence can be boosted with simple decision aids such as fast-and-frugal decision trees[114,115].</p>  
<p align="justify">Eventually, fast-and-frugal decision trees may help people establish a habit of checking epistemic cues when reading content even in the absence of a pop-up window suggesting they do so[47]</p>  
<p align="justify">Being informed about manipulative methods before encountering them online enables an individual to detect parasitic imitations of trustworthy sources and other sinister tactics[117,118].</p>  
<p align="justify">For instance, having people take on the role of a malicious influencer in a computer game has been demonstrated to improve their ability to spot and resist misinformation[61,119].</p>  
**Conclusion**  
<p align="justify">Any attempt to regulate or manage the digital world must begin with the understanding that online communication is already regulated, to some extent by public policy and laws but primarily by search engines and recommender systems whose goals and parameters may not be publicly known, let alone subject to public scrutiny. The current online environment has given rise to opaque and asymmetric relationships between users and platforms, and it is reasonable to question whether the industry will take sufficient action on its own to foster an ecosystem that values and promotes truth. The interventions we propose are aimed primarily at empowering individuals to make informed and autonomous decisions in the online ecosystem and, through their own behaviour, to foster and reinforce truth. The interventions are partly conceptualized on the basis of existing empirical findings. However, not all interventions have been tested in the specific context in which they may be deployed. It follows that some of the interventions that we have recommended, and others designed to promote the same goals, should be subject to further empirical testing. Current results identify some interventions as effective[37,119] while also indicating that others are less promising[120]. Both set of results will inform the design of more effective interventions.</p>  
<p align="justify">In our view, the future task for scientists is to design interventions that meet at least three selection criteria. They must be transparent and trustworthy to the public; standardisable within certain categories of content; and, importantly, hard to game by bad-faith actors or those with vested interests contrary to those of users or society as a whole. We also emphasize the importance of examining a wide spectrum of interventions, from nudges to boosts, to reach different types of people, who have heterogeneous preferences, motivations and online behaviours. These interventions will not completely prevent manipulation or active dissemination of false information, but they will help users recognise when malicious tactics are at work. They will also permit producers of quality information to differentiate themselves from less trustworthy sources. Behavioural interventions in the online ecology can not only inform government regulations, but also signal a platform’s commitment to truth, epistemic quality and trustworthiness. Platforms can indicate their commitment to these values by providing their users with exogenous cues and boosting and nudging interventions, and users can choose to avoid platforms that do not offer them these features.</p>  
<p align="justify">For this dynamic to gain momentum, it is not necessary that all or even the majority of users engage with nudging or boosting interventions. As the first Wikipedia contributors have proven, a critical mass may suffice to allow positive effects to scale up to major improvements. Such a dynamic may counteract a possible drawback of the proposed interventions; namely, widening information gaps between users if only empowered consumers are able to recognise quality information. If a critical mass is created, nudging and boosting interventions might well help to mitigate gaps currently arising from disparities in education or in the ability to pay for quality content. In light of the high stakes—for health, safety and self-governance itself—we err on the side of adopting interventions that empower as many people as possible.</p>
