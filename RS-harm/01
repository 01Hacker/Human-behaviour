# Recommender systems and their ethical challenges
**Abstract:**This article presents the first, systematic analysis of the ethical challenges posed by recommender systems through a literature review. The article identifies six areas of concern, and maps them onto a proposed taxonomy of different kinds of ethical impact. The analysis uncovers a gap in the literature: currently user-centred approaches do not consider the interests of a variety of other stakeholders—as opposed to just the receivers of a recommendation—in assessing the ethical impacts of a recommender system.  
为了有效地工作，推荐系统收集、管理和行动大量的个人数据。不可避免地，他们最终会塑造个人对数字环境和社会互动的体验(Burr等，2018年;德弗里斯2010;Karimi等(2018)。  
显然，无论道德问题是什么，都需要通过评估推荐系统的设计、部署和使用，以及不同利益之间的权衡来理解和解决。如果不这样做，可能会导致机会成本和问题，而这些问题本可以完全缓解或避免，反过来，公众对RS的使用产生不信任和反弹(Koene et al. 2015)。  
当前争论的碎片化可能是由于两个主要因素:一是技术的相对新，它是随着基于互联网的服务的普及而兴起的，二是20世纪90年代协同过滤技术的引入(Adomavicius和Tuzhilin 2005;Pennock等，2000年);  
同样，出于隐私方面的考虑，推荐系统提供商可能不愿分享可能危及用户个人数据的信息(Paraschakis 2018)。  
推荐系统的任务。我们所谓的推荐问题通常被总结为寻找好的项目(Jannach和Adomavicius 2016)。  
衡量推荐系统的三个指标：1.选择的空间是什么？ 2.什么才是好的选择、重要的选择？ 3.如何衡量推荐系统的性能？这些参数选择高度依赖于应用领域[LoAs，参见(Floridi 2016)]和考虑问题的抽象级别(Jannach et al. 2012).  
在新闻推荐领域，一个好的推荐可以定义为与用户相关的新闻条目(Floridi 2008)，并且可以使用点击率作为衡量系统推荐准确性的代理。  
Jameson et al.(2015)考虑了六种产生推荐的策略，它根据以下任一特征跟踪不同的选择模式:(1)带选择内容的属性;(二)选择某一内容产生的预期后果;(3)相似内容的经验优先选择;(4)关于内容的社会压力或社会信息;(五)遵守特别的的政策;(6) 试错选择。  
figure 1：推荐系统的涉众关系图  
推荐系统产生伦理影响的两种方式：1.对任何利害攸关者的效用产生(消极)影响;和/或 2.侵犯其权利。  
根据前面的分析，我们现在可以从两个维度对推荐系统引起的伦理问题进行分类(见表1):1.1. RS(给定的特征)是否会对其利益相关者的效用产生负面影响，或者相反，是否构成了侵犯权利的行为，并不一定要用效用来衡量; 2.负面影响是否构成直接损害，或是否使相关方面临未来损害或侵犯权利的风险。   
**Inappropriate content**  
早期关于伦理推荐问题的工作更多地关注于推荐的内容，并提出了根据文化和伦理偏好对系统推荐的项目进行筛选的方法。有四项研究尤其相关。Souali等人(2011)考虑了RSs在文化上不合适的问题，并提出了一个“伦理数据库”，该数据库建立在一个地区普遍接受的文化规范的基础上，作为推荐的过滤器。Tang和Winoto(2016)对这个问题采取了更动态的方法，提出了一个两层的RS，包括用户可调节的“道德过滤器”，过滤可以根据用户指定的道德偏好推荐的项目。Rodriguez和Watkins(2009)采用了一种更抽象的方法来解决伦理建议的问题，提出了一个eudaimonic RS的设想，其目的是“创造一个社会，在这个社会中，个体通过深入参与世界而体验到满足”。这两位作者预测，通过使用相互关联的大数据结构，可以实现这一点。  
最后，Paraschakis(2016, 2017, 2018)提供了最详细的描述之一。以电子商务应用为重点，Paraschakis认为有五个道德问题领域:1.用户数据收集的实践；2.公布数据；3.算法设计；4.用户接口设计；5.在线实验或A/B测试。即让选定的用户群体接受修改算法的实践，目的是从用户反馈中收集关于每个版本的有效性反馈。  
**privacy**  
至关重要的是，由于推荐系统的本质，正如我们所看到的，任何依赖于用户模型去生成个性胡推荐的方法都不仅需要考虑权衡准确性和隐私保护，还需要权衡公平性和算法的可解释性(Friedman et al. 2015; Koene et al. 2015)。因此，推荐系统的伦理分析可以通过宏观伦理方法得到更好的发展。这是一种能够考虑与数据、算法和实践相关的特定伦理问题的方法，也能够考虑这些问题如何相互关联、相互依赖和相互影响(Floridi和Taddeo 2016)。  
**Autonomy and personal identify**  
推荐系统可以侵犯个人用户的自主权，通过提供将用户推向特定方向的推荐，通过试图让他们“上瘾”某些类型的内容，或者通过限制他们暴露的选项范围(Burr等，2018;德弗里斯2010;Koene等人2015;Taddeo和Floridi, 2018年)。这些干预可以是良性的(通过过滤掉不相关的选项，使个人能动和支持更好的决策)，也可以是可疑的(说服、推动)，甚至可能是恶意的(操纵和胁迫(Burr等，2018))。  
基于聚合用户数据构建用户模型的算法分类可以再现社会类别。这可能会在建议中引入偏差。我们将在第4.4节中详细讨论这种风险。这里，重点是一组独特的问题，当用户分类算法不遵循可识别的社会类别。德弗里斯(2010)有力地阐明了这样一种观点，即我们的人格认同经历是由我们被分配到的类别所中介的。由推荐系统执行的算法分析，可能会破坏这种个人身份的个人体验，原因至少有两个。首先，推荐系统对每个用户的模型是根据其他用户与系统的交互所提供的反馈不断重新配置的。从这个意义上说，该系统不应该被概念化为跟踪一个预先建立的用户身份并针对其定制推荐，而应该是为动态地构建用户身份做出贡献(Floridi 2011)。第二,系统使用的标签分类用户可能不对应于知名的属性或用户自我认同的社会类别(例如,因为机器生成的类别可能不对应于任何已知的社会表征),所以即使用户可以访问模型的内容,他们将无法解释并连接它与他们的生活经验以一种有意义的方式。例如，对于用户来说，“狗主人”这一类别可能被认为是很重要的，而“买了一件新奇的毛衣”这一类别的社会意义就不那么重要了;然而，RS在推断用户的偏好时仍然认为它具有统计学意义。推荐系统的这些特性创造了一种环境，在这种环境中，个性化是以将用户从社交类别中移除为代价的，而社会类别有助于调节用户的身份体验。
在此背景下，一个有趣的观点是，个人自主与推荐系统的关系来自于推荐系统的“captology”。Seaver (2018a)从人类学的角度发展了这一概念: “陷阱思维”的建议在网络文化基础设施中传播，并变得几乎不可避免，它为反对自由和强迫的常见伦理框架提供了另一种选择(Seaver, 2018a)。  
推荐系统看起来就像“粘性陷阱”(我们的术语)，因为它们试图将用户“粘”在一些特定的解决方案上。这反映在Seaver所称的“魅力指标”(即衡量用户留存率的指标)中，这一指标通常被流行的推荐系统所使用。一个突出的例子是YouTube的推荐算法，该算法最近受到了很多关注，因为它倾向于推广有偏见的内容和“假新闻”，以保持用户对其平台的参与(Chaslot 2018)。将推荐系统视为陷阱，需要与用户的思想进行互动;陷阱只有在其创造者理解和配合目标的世界观和动机的情况下才会有效，这样才不会否定目标的自主能动性，而是有效地利用了它。考虑到这种captological方法，以及推荐系统陷阱的有效性和普遍性，我们要问的不是用户如何摆脱它们，而是用户如何让这些陷阱为他们工作。  
**Opacity**
从理论上讲，解释个性化推荐是如何为个人用户生成的，可以帮助降低侵犯他们自主权的风险，让他们了解为什么系统“认为”某些选项与他们相关。它还将有助于增加关于如何对用户进行分类和建模的算法决策的透明度，从而有助于防止偏见。  
设计和评估推荐系统的解释可以采取不同的形式，取决于具体的应用。正如Tintarev和Masthoff(2011)所报道的，一些研究追求以用户为中心的评估指标，包括评估建议解释的指标。一个好的解释取决于几个标准:推荐给用户的目的;该解释是否与产生该建议的机制准确相符;是否提高了系统的透明度和可预测性;以及它是否能帮助用户更有效地(如更快地)做决定，还是更有效地(如提高满意度)做决定。  
**Fairness**  
公平在算法决策中是一个广泛的问题，由于存在多种公平概念，而这些概念并不都是相互兼容的，使得问题更加复杂(Friedler et al. 2016)。  
**Social effects**  
一些推荐系统的一个被广泛讨论的影响是它们对社会的变革性影响。特别是新闻推荐系统和社交媒体过滤器,通过自然的设计,运行的风险绝缘用户接触不同的观点,创建自我强化的偏见和“过滤泡沫”损害公众辩论的正常功能,小组审议,民主制度更普遍(Bozdag 2013;Bozdag和van den Hoven, 2015年;Harambam等人2018年;Helberger et al. 2016;Koene等人2015;Reviglio 2017;(Zook等，2017)。推荐系统的这一特性会对社会效用产生负面影响。一个相对近期但令人担忧的例子是反对疫苗的宣传传播，这已经被认为与群体免疫力下降有关(Burki 2019)。  
一个密切相关的问题是，如何保护这些系统不被(有时甚至是很小的)活跃用户群体操纵，这些用户与系统的互动可以产生强烈的积极反馈，从而提高系统对特定项目的推荐率(Chakraborty et al. 2019)。新闻推荐系统、流媒体平台和社交网络可以成为有针对性的政治宣传的舞台，最近2018年剑桥分析丑闻和近年来美国政治选举中有记录的外部干预就是证明(Howard et al. 2019)。  
关于这一主题的文献提出了一系列增加建议多样性的方法。几位作者指出，尤其是新闻推荐系统，在基于预先指定的用户偏好或行为数据生成个性化推荐时，必须在对用户的预期相关性和多样性之间达成权衡(Helberger et al. 2016;Reviglio 2017)。在这方面，Bozdag和van den Hoven(2015)认为，对抗信息隔离的算法工具的设计应该对这些工具中隐含的民主规范更加敏感。  
一般来说，解决两极分化和社会可操控性问题的方法似乎分为自下而上和自上而下两种策略，优先考虑用户的偏好(以及用户在决定如何配置个性化推荐时的自主权)或平衡公共领域的社会偏好。同样，一些作者采取了坚决的以用户为中心的观点。例如，Harambam等人(2018)提出使用不同的“推荐人物”，或“预先配置和拟人化类型的推荐算法”，表达不同的用户偏好，包括新颖性、多样性、相关性和推荐算法的其他属性。本着同样的精神，Reviglio(2017)强调了促进意外发现的重要性，即使以牺牲用户体验的某些方面为代价，比如降低推荐的相关性。  
**Discussion**  
不公平的建议可能会对效用产生负面影响，但Yao和Huang(2017)也指出，公平和效用是相互独立的，不公平最好被归为直接侵犯权利的一种类型。  
